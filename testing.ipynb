{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.feature_extraction import image #reconstract from patches 2d\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import torchvision.utils\n",
    "from collections import defaultdict\n",
    "import tifffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)\n",
    "PATCH_SIZE = 128\n",
    "STRIDE = 32\n",
    "BATCH_SIZE = 40\n",
    "NUM_EPOCHS = 2\n",
    "NUM_WORKERS = 2\n",
    "IMAGE_HEIGHT = 512 \n",
    "IMAGE_WIDTH = 512\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "TRAIN_IMG_DIR = \"/home/jessica/Project-Code/OpenSprayerSeg/train/\"\n",
    "TRAIN_MASK_DIR = \"/home/jessica/Project-Code/OpenSprayerSeg/train_labels/\"\n",
    "VALIDATION_IMG_DIR = \"/home/jessica/Project-Code/OpenSprayerSeg/val/\"\n",
    "VALIDATION_MASK_DIR = \"/home/jessica/Project-Code/OpenSprayerSeg/val_labels/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    When looking at the model for how a UNET architecture is set up, we see that for all of the \"steps\" the image gets \n",
    "    'transformed' two times. This is called a convolution.\n",
    "        CONVOLUTION: a multiplication that is performed between an array of input data and a two-dimensional array of weights\n",
    "\n",
    "    This is the function that creates the double convolution that brings, as describes in the uNet_arctutrecture png that moves the information, in one group\n",
    "    from the left to the right.\n",
    "\n",
    "    nn.Conv2d((in_channels, out_channels, kernel_size, stride, padding, bias)\n",
    "        - in_channels: the number of channels in the input image, (colored images are 3, black and white is 1(only color in 1 dimension))\n",
    "        - out_channels: number of channels produced by the convolution\n",
    "        - kernel_size: widthxheight of the mask (this moves over the image) an int means that the kernal matrix is a nxn matrix and a tuple means a nxm\n",
    "        - stride: how far the kernal moves each time is does, 1 mean it moves over to the next pixel\n",
    "        - padding: a 1 means \"same\" the input hight and width will be the smae after the covolution\n",
    "        - bias: do we want a learnable bias (do we want to keep the same value throughout the model?)\n",
    "\"\"\"\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels), #batchnorm cancels out the bias from conv2d so we can prevent a \"unless\" parameter by setting bias = False\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNET(nn.Module):\n",
    "    \"\"\"\n",
    "        This is the function that actually builds the movment up and down that is needed for analysing and doing the Neural Network stuff that we want\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, out_channels=1, features=[8, 16, 32, 64]):\n",
    "        super(UNET, self).__init__()\n",
    "\n",
    "        #we want to be able to do model.eval and for the batch normal layers so that is why we choe nn.ModuleList\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Down part of UNET\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature)) #matches some input to the next size ---> the first time this ever runs it matches 3 to 64\n",
    "            in_channels = feature #this changes the number of inputs so we have a 1 --> 64 --> 128 situtaion so the model looks correct\n",
    "\n",
    "        # Up part of UNET\n",
    "        for feature in reversed(features):\n",
    "            #this is the movement from left to right via the gray arrow in which the in_channels on the right side are double the size of the number of features on the left\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    feature*2, feature, kernel_size=2, stride=2, #the kernel_size being 2 doubles the size (heigh and width) of the image\n",
    "                )\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feature*2, feature)) #this because in each \"group\" we move over 2 and up one\n",
    "       \n",
    "        #bottleneck (the turning point from down to up)\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        #final convolution on the top right which keeps the same size of the image but decreaes the out_channels which would provide our final predicted result\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        \n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x) #the ordering is important here as we move sideways from lowest resolution first than to highest resultion\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2): # up than double conv\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2] #we have a step of 2 and we want a linear step of 1 ordering\n",
    "\n",
    "            #if the two are not the same size, this is important since otherwise it will not work, we will take out height and width\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx+1](concat_skip) #double convolutopm\n",
    "\n",
    "        return self.final_conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform = None, patches = None, patch_size = 64, stride = 32):\n",
    "        self.image_dir   = image_dir\n",
    "        self.mask_dir    = mask_dir\n",
    "        self.transform   = transform\n",
    "        self.patches     = patches\n",
    "        self.patch_size  = patch_size\n",
    "        self.stride      = stride\n",
    "        #list all files that are in that folder\n",
    "        self.images      = os.listdir(image_dir)\n",
    "\n",
    "    #get how many images are in a specified directory\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    #get a specified item in images at a specified index\n",
    "    def __getitem__(self, index):\n",
    "        image_path = os.path.join(self.image_dir, self.images[index])\n",
    "        mask_path  = os.path.join(self.mask_dir, self.images[index])\n",
    "        \n",
    "        #loading the images\n",
    "        image = np.array(Image.open(image_path).convert(\"L\"), dtype=np.float32)\n",
    "        mask  = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            augumentation = self.transform(image = image, mask = mask)\n",
    "            image = augumentation[\"image\"]\n",
    "            mask  = augumentation[\"mask\"]\n",
    "\n",
    "        if self.patches is not None:\n",
    "            image = torch.from_numpy(image).unsqueeze(0).unsqueeze(0)\n",
    "            mask = torch.from_numpy(mask).unsqueeze(0).unsqueeze(0)\n",
    "            image = image.unfold(2, self.patch_size, self.stride).unfold(3, self.patch_size, self.stride)\n",
    "            mask  = mask.unfold(2, self.patch_size, self.stride).unfold(3, self.patch_size, self.stride)\n",
    "\n",
    "        return [image, mask] \n",
    "    \n",
    "    def extract_patches(self, image, patch_size, stride): #for when I want to make a custom function\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint, model):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_MODEL:\n",
    "    load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = CarDataset(image_dir=TRAIN_IMG_DIR, mask_dir=TRAIN_MASK_DIR, transform= None, patches = True, patch_size=PATCH_SIZE, stride = STRIDE)\n",
    "val_ds = CarDataset(image_dir=VALIDATION_IMG_DIR,mask_dir=VALIDATION_MASK_DIR, transform=None, patches = True, patch_size=PATCH_SIZE, stride = STRIDE)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, shuffle=True)\n",
    "val_loader = DataLoader(val_ds,batch_size=BATCH_SIZE,num_workers=NUM_WORKERS,pin_memory=PIN_MEMORY,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNET(\n",
      "  (ups): ModuleList(\n",
      "    (0): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): DoubleConv(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (3): DoubleConv(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (4): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (5): DoubleConv(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (6): ConvTranspose2d(16, 8, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (7): DoubleConv(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (downs): ModuleList(\n",
      "    (0): DoubleConv(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (1): DoubleConv(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): DoubleConv(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (3): DoubleConv(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (bottleneck): DoubleConv(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (final_conv): Conv2d(8, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = UNET(in_channels=1, out_channels=1).to(DEVICE) #if we wanted multisegmentation, we would change the number of output channels to the correct number of \"classes\"\n",
    "print(model)\n",
    "#binary cross entropy - we choose this beacuse we are not doing sigmoid for the output\n",
    "loss_fn = nn.BCEWithLogitsLoss() #we would change this to CEEithLogitsLoss() if multi classes\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model #not correct anymore\n",
    "\n",
    "def train_fn(train_data, model, optimizer, loss, scaler, device):\n",
    "    loop = tqdm(train_data)\n",
    "    for _, (specific_image, specific_mask) in enumerate(loop):\n",
    "        specific_image = specific_image.contiguous().view(-1, 64, 64)\n",
    "        specific_mask = specific_mask.contiguous().view(-1, 64, 64)\n",
    "\n",
    "        for img_patch, mask_patch in zip(specific_image, specific_mask):\n",
    "            img_patch = img_patch.unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "            mask_patch = mask_patch.unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "            #going forwards in the model\n",
    "            with torch.cuda.amp.autocast(): #ueses float16\n",
    "                predicted = model(img_patch)\n",
    "                loss = loss_fn(predicted, mask_patch)\n",
    "\n",
    "            #back progagation\n",
    "            optimizer.zero_grad() #zeros the gradients from the previous\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            #update tqdm loop\n",
    "            loop.set_postfix(loss = loss.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see how good the model is #not correct anymore\n",
    "def accuracy_of_model(loader, model, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _, (specific_image, specific_mask) in enumerate(loader):\n",
    "            num_correct = 0\n",
    "            num_pixels = 0\n",
    "            dice_score = 0\n",
    "\n",
    "            specific_image = specific_image.contiguous().view(-1, PATCH_SIZE, PATCH_SIZE)\n",
    "            specific_mask = specific_mask.contiguous().view(-1, PATCH_SIZE, PATCH_SIZE)\n",
    "            \n",
    "            for img_patch, mask_patch in zip(specific_image, specific_mask):\n",
    "                img_patch = img_patch.unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "                mask_patch = mask_patch.unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "                preds = torch.sigmoid(model(img_patch))\n",
    "                preds = (preds > 0.5).float()\n",
    "\n",
    "                num_correct += (preds == mask_patch).sum() #sums all of the pixels\n",
    "                num_pixels += torch.numel(preds) #gets the number of pixels\n",
    "                dice_score += (2 * (preds * mask_patch).sum()) / ( (preds + mask_patch).sum() + 1e-8 ) # a better way of seeing how accurate your prediction is. This is needed since if it makes it all bblack pixels it would be auto 80% correct --> just for binary\n",
    "\n",
    "            print(f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f} for image group\")\n",
    "            print(f\"Dice score for group: {dice_score/len(loader)}\")\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions_as_images(loader, model, patch_size, stride, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    processed_patches = []\n",
    "    for idx, (specific_image, specific_mask) in enumerate(loader):\n",
    "        processed_patches_per_image = []\n",
    "        specific_image = specific_image.contiguous().view(-1, PATCH_SIZE, PATCH_SIZE) #makes into \n",
    "        specific_mask = specific_mask.contiguous().view(-1, PATCH_SIZE, PATCH_SIZE)\n",
    "        for img_patch, mask_patch in zip(specific_image, specific_mask):\n",
    "            img_patch = img_patch.unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "            mask_patch = mask_patch.unsqueeze(0).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                preds = torch.sigmoid(model(img_patch))\n",
    "                preds = (preds > 0.5).float()\n",
    "            processed_patches_per_image.append(preds) #per each patch for a specific image\n",
    "        processed_patches.append(torch.stack(processed_patches_per_image)) #combines all patches to a single tensor, each index of the array is a seperate image.\n",
    "    reconstruct_patches_to_image(processed_patches, patch_size, stride, device)\n",
    "    \n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_original_image_demensions(stacked_patch):\n",
    "    num_patches, _, _, patch_h, patch_w = stacked_patch.shape \n",
    "\n",
    "    # Calculate the number of patches along height and width\n",
    "    num_patches_per_dim = int(math.sqrt(num_patches))\n",
    "\n",
    "    # Calculate the original image dimensions\n",
    "    original_height = (num_patches_per_dim - 1) * 16 + patch_h\n",
    "    original_width = (num_patches_per_dim - 1) * 16 + patch_w\n",
    "    \n",
    "    return original_height, original_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_patches_to_image(array_of_images, patch_size, stride, device):\n",
    "    #loops through each image for recombining\n",
    "    index = 0\n",
    "    for image in array_of_images:\n",
    "        height, width = _get_original_image_demensions(image)\n",
    "        reconstructed = torch.zeros((channels, height, width), dtype=image.dtype).to(device)\n",
    "        count_matrix  = torch.zeros((channels, height, width), dtype=image.dtype).to(DEVICE)\n",
    "\n",
    "        # Calculate the number of patches along height and width\n",
    "        num_patches_h = (height - patch_size) // stride + 1\n",
    "        num_patches_w = (height - patch_size) // stride + 1\n",
    "\n",
    "        # Place the patches back into the image\n",
    "        patch_idx = 0\n",
    "        for i in range(num_patches_h):\n",
    "            for j in range(num_patches_w):\n",
    "                h_start = i * stride\n",
    "                w_start = j * stride\n",
    "                reconstructed[:, h_start:h_start+patch_size, w_start:w_start+patch_size] += image[patch_idx].squeeze(1)\n",
    "                count_matrix[:, h_start:h_start+patch_size, w_start:w_start+patch_size] += 1\n",
    "                patch_idx += 1\n",
    "        \n",
    "        reconstructed /= count_matrix\n",
    "        reconstructed_image_np = reconstructed.squeeze(0).cpu().numpy()\n",
    "        str_test = \"test_\" + str(index) + \".tif\"\n",
    "        # Plot the reconstructed image\n",
    "        tifffile.imwrite(str_test, reconstructed_image_np)\n",
    "        index +=1\n",
    "    return \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 2061/107986944 with acc 0.00 for image group\n",
      "Dice score for group: 11950.3623046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [05:09<00:00, 309.69s/it, loss=-3.51e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 0/107986944 with acc 0.00 for image group\n",
      "Dice score for group: 13024.224609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [05:09<00:00, 309.66s/it, loss=-1.12e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 0/107986944 with acc 0.00 for image group\n",
      "Dice score for group: 13024.224609375\n"
     ]
    }
   ],
   "source": [
    "accuracy_of_model(val_loader, model, device = DEVICE)\n",
    "for epochs in range(NUM_EPOCHS): #how many times do we want to train?\n",
    "    # save_checkpoint(checkpoint)\n",
    "    train_fn(train_loader, model, optimizer, loss_fn, scaler, device = DEVICE)\n",
    "    accuracy_of_model(val_loader, model, device = DEVICE)\n",
    "save_predictions_as_images(val_loader, model = model, patch_size = PATCH_SIZE, stride = STRIDE, device=DEVICE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xraydiffraction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
